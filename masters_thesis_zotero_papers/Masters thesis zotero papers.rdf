<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#">
    <rdf:Description rdf:about="http://arxiv.org/abs/2005.01643">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levine</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>Aviral</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tucker</foaf:surname>
                        <foaf:givenName>George</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fu</foaf:surname>
                        <foaf:givenName>Justin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_73"/>
        <link:link rdf:resource="#item_74"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems</dc:title>
        <dcterms:abstract>In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.</dcterms:abstract>
        <dc:date>2020-11-01</dc:date>
        <z:shortTitle>Offline Reinforcement Learning</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2005.01643</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-04-17 09:14:14</dcterms:dateSubmitted>
        <dc:description>arXiv:2005.01643 [cs, stat]</dc:description>
        <prism:number>arXiv:2005.01643</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_73">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/73/2005.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2005.01643</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-04-17 09:14:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_74">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/74/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2005.01643.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-04-17 09:14:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2102.02915">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0278-3649,%201741-3176"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ibarz</foaf:surname>
                        <foaf:givenName>Julian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>Jie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Finn</foaf:surname>
                        <foaf:givenName>Chelsea</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kalakrishnan</foaf:surname>
                        <foaf:givenName>Mrinal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pastor</foaf:surname>
                        <foaf:givenName>Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levine</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_78"/>
        <link:link rdf:resource="#item_79"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>How to Train Your Robot with Deep Reinforcement Learning; Lessons We've Learned</dc:title>
        <dcterms:abstract>Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world. At the same time,real world robotics provides an appealing domain for evaluating such algorithms, as it connects directly to how humans learn; as an embodied agent in the real world. Learning to perceive and move in the real world presents numerous challenges, some of which are easier to address than others, and some of which are often not considered in RL research that focuses only on simulated domains. In this review article, we present a number of case studies involving robotic deep RL. Building off of these case studies, we discuss commonly perceived challenges in deep RL and how they have been addressed in these works. We also provide an overview of other outstanding challenges, many of which are unique to the real-world robotics setting and are not often the focus of mainstream RL research. Our goal is to provide a resource both for roboticists and machine learning researchers who are interested in furthering the progress of deep RL in the real world.</dcterms:abstract>
        <dc:date>04/2021</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2102.02915</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-04-17 09:14:27</dcterms:dateSubmitted>
        <dc:description>arXiv:2102.02915 [cs]</dc:description>
        <bib:pages>698-721</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0278-3649,%201741-3176">
        <prism:volume>40</prism:volume>
        <dc:title>The International Journal of Robotics Research</dc:title>
        <dc:identifier>DOI 10.1177/0278364920987859</dc:identifier>
        <prism:number>4-5</prism:number>
        <dcterms:alternative>The International Journal of Robotics Research</dcterms:alternative>
        <dc:identifier>ISSN 0278-3649, 1741-3176</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_78">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/78/2102.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2102.02915</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-04-17 09:14:33</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_79">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/79/Ibarz et al. - 2021 - How to Train Your Robot with Deep Reinforcement Le.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2102.02915.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-04-17 09:14:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2108.03298">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mandlekar</foaf:surname>
                        <foaf:givenName>Ajay</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Danfei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wong</foaf:surname>
                        <foaf:givenName>Josiah</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nasiriany</foaf:surname>
                        <foaf:givenName>Soroush</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Chen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kulkarni</foaf:surname>
                        <foaf:givenName>Rohun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fei-Fei</foaf:surname>
                        <foaf:givenName>Li</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Savarese</foaf:surname>
                        <foaf:givenName>Silvio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>Yuke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Martín-Martín</foaf:surname>
                        <foaf:givenName>Roberto</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_77"/>
        <link:link rdf:resource="#item_80"/>
        <link:link rdf:resource="#item_81"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>What Matters in Learning from Offline Human Demonstrations for Robot Manipulation</dc:title>
        <dcterms:abstract>Imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. While recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. In this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. Our study analyzes the most critical challenges when learning from offline human data for manipulation. Based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. We also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. We have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data. Codebase, datasets, trained models, and more available at https://arise-initiative.github.io/robomimic-web/</dcterms:abstract>
        <dc:date>2021-09-24</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2108.03298</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-04-17 09:14:32</dcterms:dateSubmitted>
        <dc:description>arXiv:2108.03298 [cs]</dc:description>
        <prism:number>arXiv:2108.03298</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_77">
       <rdf:value>Comment: CoRL 2021 (Oral)</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_80">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/80/2108.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2108.03298</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-04-17 09:14:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_81">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/81/Mandlekar et al. - 2021 - What Matters in Learning from Offline Human Demons.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2108.03298.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-04-17 09:14:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1806.10293">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kalashnikov</foaf:surname>
                        <foaf:givenName>Dmitry</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Irpan</foaf:surname>
                        <foaf:givenName>Alex</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pastor</foaf:surname>
                        <foaf:givenName>Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ibarz</foaf:surname>
                        <foaf:givenName>Julian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Herzog</foaf:surname>
                        <foaf:givenName>Alexander</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jang</foaf:surname>
                        <foaf:givenName>Eric</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Quillen</foaf:surname>
                        <foaf:givenName>Deirdre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Holly</foaf:surname>
                        <foaf:givenName>Ethan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kalakrishnan</foaf:surname>
                        <foaf:givenName>Mrinal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vanhoucke</foaf:surname>
                        <foaf:givenName>Vincent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levine</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_86"/>
        <link:link rdf:resource="#item_87"/>
        <link:link rdf:resource="#item_88"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation</dc:title>
        <dcterms:abstract>In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.</dcterms:abstract>
        <dc:date>2018-11-27</dc:date>
        <z:shortTitle>QT-Opt</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1806.10293</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-07 09:54:44</dcterms:dateSubmitted>
        <dc:description>arXiv:1806.10293 [cs, stat]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1806.10293</dc:identifier>
        <prism:number>arXiv:1806.10293</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_86">
        <rdf:value>Comment: CoRL 2018 camera ready. 23 pages, 14 figures</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_87">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/87/Kalashnikov et al. - 2018 - QT-Opt Scalable Deep Reinforcement Learning for V.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1806.10293.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-07 09:54:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_88">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/88/1806.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1806.10293</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-07 09:54:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1801.01290">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Haarnoja</foaf:surname>
                        <foaf:givenName>Tuomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Aurick</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Abbeel</foaf:surname>
                        <foaf:givenName>Pieter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levine</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_90"/>
        <link:link rdf:resource="#item_91"/>
        <link:link rdf:resource="#item_92"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</dc:title>
        <dcterms:abstract>Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.</dcterms:abstract>
        <dc:date>2018-08-08</dc:date>
        <z:shortTitle>Soft Actor-Critic</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1801.01290</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-07 09:57:26</dcterms:dateSubmitted>
        <dc:description>arXiv:1801.01290 [cs, stat]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1801.01290</dc:identifier>
        <prism:number>arXiv:1801.01290</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_90">
        <rdf:value>Comment: ICML 2018 Videos: sites.google.com/view/soft-actor-critic Code: github.com/haarnoja/sac</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_91">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/91/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1801.01290.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-07 09:57:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_92">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/92/1801.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1801.01290</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-07 09:57:33</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2203.01387">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2162-237X,%202162-2388"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Prudencio</foaf:surname>
                        <foaf:givenName>Rafael Figueiredo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maximo</foaf:surname>
                        <foaf:givenName>Marcos R. O. A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Colombini</foaf:surname>
                        <foaf:givenName>Esther Luna</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_94"/>
        <link:link rdf:resource="#item_95"/>
        <link:link rdf:resource="#item_96"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems</dc:title>
        <dcterms:abstract>With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks' properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:shortTitle>A Survey on Offline Reinforcement Learning</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2203.01387</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-07 10:13:26</dcterms:dateSubmitted>
        <dc:description>arXiv:2203.01387 [cs, stat]</dc:description>
        <bib:pages>1-0</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2162-237X,%202162-2388">
        <dc:title>IEEE Transactions on Neural Networks and Learning Systems</dc:title>
        <dc:identifier>DOI 10.1109/TNNLS.2023.3250269</dc:identifier>
        <dcterms:alternative>IEEE Trans. Neural Netw. Learning Syst.</dcterms:alternative>
        <dc:identifier>ISSN 2162-237X, 2162-2388</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_94">
        <rdf:value>Comment: 21 pages; Final version accepted to IEEE Transactions on Neural Networks and Learning Systems</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_95">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/95/Prudencio et al. - 2024 - A Survey on Offline Reinforcement Learning Taxono.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2203.01387.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-07 10:13:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_96">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/96/2203.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2203.01387</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-07 10:13:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/10129251">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2162-2388"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Min</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Zhongqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xia</foaf:surname>
                        <foaf:givenName>Yuanqing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Jinhui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_101"/>
        <link:link rdf:resource="#item_100"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Convergence</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Costs</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Discrete-time systems</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>model predictive control</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>policy iteration (PI)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Predictive control</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>reinforcement learning (RL)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Stability criteria</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Sun</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Task analysis</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Reinforcement Learning-Based Model Predictive Control for Discrete-Time Systems</dc:title>
        <dcterms:abstract>This article proposes a novel reinforcement learning-based model predictive control (RLMPC) scheme for discrete-time systems. The scheme integrates model predictive control (MPC) and reinforcement learning (RL) through policy iteration (PI), where MPC is a policy generator and the RL technique is employed to evaluate the policy. Then the obtained value function is taken as the terminal cost of MPC, thus improving the generated policy. The advantage of doing so is that it rules out the need for the offline design paradigm of the terminal cost, the auxiliary controller, and the terminal constraint in traditional MPC. Moreover, RLMPC proposed in this article enables a more flexible choice of prediction horizon due to the elimination of the terminal constraint, which has great potential in reducing the computational burden. We provide a rigorous analysis of the convergence, feasibility, and stability properties of RLMPC. Simulation results show that RLMPC achieves nearly the same performance as traditional MPC in the control of linear systems and exhibits superiority over traditional MPC for nonlinear ones.</dcterms:abstract>
        <dc:date>2024-03</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10129251</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:27:58</dcterms:dateSubmitted>
        <dc:description>Conference Name: IEEE Transactions on Neural Networks and Learning Systems</dc:description>
        <bib:pages>3312-3324</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2162-2388">
        <prism:volume>35</prism:volume>
        <dc:title>IEEE Transactions on Neural Networks and Learning Systems</dc:title>
        <dc:identifier>DOI 10.1109/TNNLS.2023.3273590</dc:identifier>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 2162-2388</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_101">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/101/10129251.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10129251</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:28:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_100">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/100/Lin et al. - 2024 - Reinforcement Learning-Based Model Predictive Cont.pdf"/>
        <dc:title>IEEE Xplore Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10129251&amp;ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEwMTI5MjUx</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:28:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://iopscience.iop.org/article/10.1088/1742-6596/2203/1/012058">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1742-6588,%201742-6596"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pan</foaf:surname>
                        <foaf:givenName>Xia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Xiaowei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Qingyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Nannan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_103"/>
        <dc:title>Model Predictive Control : A Reinforcement Learning-based Approach</dc:title>
        <dcterms:abstract>This article proposes a method of model predictive control, which combine the excellent data-driven optimization ability of reinforcement learning and model predictive control to design the controller. Different from the off-line design of MPC, reinforcement learning is based on the adaptation of on-line data to achieve the purpose of control strategy optimization. The reinforcement learning-based model predictive control can improve the control performance effectively. And the numerical simulations are given to demonstrate the effectiveness of the proposed approach.</dcterms:abstract>
        <dc:date>2022-02-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Model Predictive Control</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://iopscience.iop.org/article/10.1088/1742-6596/2203/1/012058</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:30:08</dcterms:dateSubmitted>
        <bib:pages>012058</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1742-6588,%201742-6596">
        <prism:volume>2203</prism:volume>
        <dc:title>Journal of Physics: Conference Series</dc:title>
        <dc:identifier>DOI 10.1088/1742-6596/2203/1/012058</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>J. Phys.: Conf. Ser.</dcterms:alternative>
        <dc:identifier>ISSN 1742-6588, 1742-6596</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_103">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/103/Pan et al. - 2022 - Model Predictive Control  A Reinforcement Learnin.pdf"/>
        <dc:title>Pan et al. - 2022 - Model Predictive Control  A Reinforcement Learnin.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://iopscience.iop.org/article/10.1088/1742-6596/2203/1/012058/pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:29:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1904.04152">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0018-9286,%201558-2523,%202334-3303"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gros</foaf:surname>
                        <foaf:givenName>Sébastien</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zanon</foaf:surname>
                        <foaf:givenName>Mario</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_106"/>
        <link:link rdf:resource="#item_107"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Electrical Engineering and Systems Science - Systems and Control</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Data-driven Economic NMPC using Reinforcement Learning</dc:title>
        <dcterms:abstract>Reinforcement Learning (RL) is a powerful tool to perform data-driven optimal control without relying on a model of the system. However, RL struggles to provide hard guarantees on the behavior of the resulting control scheme. In contrast, Nonlinear Model Predictive Control (NMPC) and Economic NMPC (ENMPC) are standard tools for the closed-loop optimal control of complex systems with constraints and limitations, and benefit from a rich theory to assess their closed-loop behavior. Unfortunately, the performance of (E)NMPC hinges on the quality of the model underlying the control scheme. In this paper, we show that an (E)NMPC scheme can be tuned to deliver the optimal policy of the real system even when using a wrong model. This result also holds for real systems having stochastic dynamics. This entails that ENMPC can be used as a new type of function approximator within RL. Furthermore, we investigate our results in the context of ENMPC and formally connect them to the concept of dissipativity, which is central for the ENMPC stability. Finally, we detail how these results can be used to deploy classic RL tools for tuning (E)NMPC schemes. We apply these tools on both a classical linear MPC setting and a standard nonlinear example from the ENMPC literature.</dcterms:abstract>
        <dc:date>2/2020</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1904.04152</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:33:29</dcterms:dateSubmitted>
        <dc:description>arXiv:1904.04152 [cs]</dc:description>
        <bib:pages>636-648</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0018-9286,%201558-2523,%202334-3303">
        <prism:volume>65</prism:volume>
        <dc:title>IEEE Transactions on Automatic Control</dc:title>
        <dc:identifier>DOI 10.1109/TAC.2019.2913768</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>IEEE Trans. Automat. Contr.</dcterms:alternative>
        <dc:identifier>ISSN 0018-9286, 1558-2523, 2334-3303</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_106">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/106/Gros and Zanon - 2020 - Data-driven Economic NMPC using Reinforcement Lear.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1904.04152.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:33:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_107">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/107/1904.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1904.04152</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:33:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2301.01667">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sawant</foaf:surname>
                        <foaf:givenName>Shambhuraj</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anand</foaf:surname>
                        <foaf:givenName>Akhil S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reinhardt</foaf:surname>
                        <foaf:givenName>Dirk</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gros</foaf:surname>
                        <foaf:givenName>Sebastien</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_110"/>
        <link:link rdf:resource="#item_111"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Electrical Engineering and Systems Science - Systems and Control</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Learning-based MPC from Big Data Using Reinforcement Learning</dc:title>
        <dcterms:abstract>This paper presents an approach for learning Model Predictive Control (MPC) schemes directly from data using Reinforcement Learning (RL) methods. The state-of-the-art learning methods use RL to improve the performance of parameterized MPC schemes. However, these learning algorithms are often gradient-based methods that require frequent evaluations of computationally expensive MPC schemes, thereby restricting their use on big datasets. We propose to tackle this issue by using tools from RL to learn a parameterized MPC scheme directly from data in an offline fashion. Our approach derives an MPC scheme without having to solve it over the collected dataset, thereby eliminating the computational complexity of existing techniques for big data. We evaluate the proposed method on three simulated experiments of varying complexity.</dcterms:abstract>
        <dc:date>2023-01-04</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2301.01667</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:36:38</dcterms:dateSubmitted>
        <dc:description>arXiv:2301.01667 [cs, eess]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2301.01667</dc:identifier>
        <prism:number>arXiv:2301.01667</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_110">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/110/Sawant et al. - 2023 - Learning-based MPC from Big Data Using Reinforceme.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2301.01667.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:36:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_111">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/111/2301.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2301.01667</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-13 08:36:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S2405896323009151">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2405-8963"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kordabad</foaf:surname>
                        <foaf:givenName>Arash Bahari</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reinhardt</foaf:surname>
                        <foaf:givenName>Dirk</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anand</foaf:surname>
                        <foaf:givenName>Akhil S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gros</foaf:surname>
                        <foaf:givenName>Sebastien</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Learning for MPC</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>MPC</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Reinforcement Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Stability &amp; Safety</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Reinforcement Learning for MPC: Fundamentals and Current Challenges</dc:title>
        <dcterms:abstract>Recent publications have laid a solid theoretical foundation for the combination of Reinforcement Learning and Model Predictive Control, in view of obtaining high-performance data-driven MPC policies. Early practical results, both in simulation and in experiments, have shown the potential of this combination but have also revealed certain challenges. In addition, the technical complexity of these results makes it difficult for interested readers to gather the fundamental ideas and principles behind this combination. This paper aims to provide a coherent and more accessible picture of these results and to offer significantly deeper and more mature insights into their meaning than has been proposed before. It also aims at identifying the current challenges in the field.</dcterms:abstract>
        <dc:date>2023-01-01</dc:date>
        <z:shortTitle>Reinforcement Learning for MPC</z:shortTitle>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S2405896323009151</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-20 11:11:56</dcterms:dateSubmitted>
        <bib:pages>5773-5780</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2405-8963">
        <dcterms:isPartOf>
           <bib:Series><dc:title>22nd IFAC World Congress</dc:title></bib:Series>
        </dcterms:isPartOf>
        <prism:volume>56</prism:volume>
        <dc:title>IFAC-PapersOnLine</dc:title>
        <dc:identifier>DOI 10.1016/j.ifacol.2023.10.548</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>IFAC-PapersOnLine</dcterms:alternative>
        <dc:identifier>ISSN 2405-8963</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="#item_116">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Seborg</foaf:surname>
                        <foaf:givenName>Dale E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_115"/>
        <dc:title>Process Dynamics and Control</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_115">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/115/Seborg - Process Dynamics and Control.pdf"/>
        <dc:title>Seborg - Process Dynamics and Control.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://elmoukrie.com/wp-content/uploads/2022/06/process-dynamics-and-control-dale-e.-seborg-thomas-f.-edgar-etc.-z-lib.org_.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-20 15:12:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Book rdf:about="urn:isbn:978-0-262-03924-6">
        <z:itemType>book</z:itemType>
        <dcterms:isPartOf>
            <bib:Series>
                <dc:title>Adaptive computation and machine learning series</dc:title>
            </bib:Series>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cambridge, Massachusetts</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>The MIT Press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sutton</foaf:surname>
                        <foaf:givenName>Richard S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barto</foaf:surname>
                        <foaf:givenName>Andrew G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Reinforcement learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Reinforcement learning: an introduction</dc:title>
        <dcterms:abstract>&quot;Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms.&quot;--</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:shortTitle>Reinforcement learning</z:shortTitle>
        <z:libraryCatalog>Library of Congress ISBN</z:libraryCatalog>
        <dc:subject>
           <dcterms:LCC><rdf:value>Q325.6 .R45 2018</rdf:value></dcterms:LCC>
        </dc:subject>
        <dc:identifier>ISBN 978-0-262-03924-6</dc:identifier>
        <prism:edition>Second edition</prism:edition>
        <z:numPages>526</z:numPages>
    </bib:Book>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/6386025">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2012.6386025</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tassa</foaf:surname>
                        <foaf:givenName>Yuval</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Erez</foaf:surname>
                        <foaf:givenName>Tom</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Todorov</foaf:surname>
                        <foaf:givenName>Emanuel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_137"/>
        <link:link rdf:resource="#item_136"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computational modeling</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Heuristic algorithms</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Mathematical model</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Optimization</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Real-time systems</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Robots</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Trajectory</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Synthesis and stabilization of complex behaviors through online trajectory optimization</dc:title>
        <dcterms:abstract>We present an online trajectory optimization method and software platform applicable to complex humanoid robots performing challenging tasks such as getting up from an arbitrary pose on the ground and recovering from large disturbances using dexterous acrobatic maneuvers. The resulting behaviors, illustrated in the attached video, are computed only 7 × slower than real time, on a standard PC. The video also shows results on the acrobot problem, planar swimming and one-legged hopping. These simpler problems can already be solved in real time, without pre-computing anything.</dcterms:abstract>
        <dc:date>2012-10</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/6386025</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:42:14</dcterms:dateSubmitted>
        <dc:description>ISSN: 2153-0866</dc:description>
        <bib:pages>4906-4913</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_137">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/137/6386025.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/6386025</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:42:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_136">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/136/Tassa et al. - 2012 - Synthesis and stabilization of complex behaviors t.pdf"/>
        <dc:title>IEEE Xplore Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=6386025&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:42:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1708.02596">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nagabandi</foaf:surname>
                        <foaf:givenName>Anusha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kahn</foaf:surname>
                        <foaf:givenName>Gregory</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fearing</foaf:surname>
                        <foaf:givenName>Ronald S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levine</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_139"/>
        <link:link rdf:resource="#item_140"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning</dc:title>
        <dcterms:abstract>Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf</dcterms:abstract>
        <dc:date>2017-12-01</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1708.02596</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:43:16</dcterms:dateSubmitted>
        <dc:description>arXiv:1708.02596 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1708.02596</dc:identifier>
        <prism:number>arXiv:1708.02596</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_139">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/139/Nagabandi et al. - 2017 - Neural Network Dynamics for Model-Based Deep Reinf.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1708.02596.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:43:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_140">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/140/1708.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1708.02596</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:43:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1805.12114">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chua</foaf:surname>
                        <foaf:givenName>Kurtland</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Calandra</foaf:surname>
                        <foaf:givenName>Roberto</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McAllister</foaf:surname>
                        <foaf:givenName>Rowan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levine</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_142"/>
        <link:link rdf:resource="#item_143"/>
        <link:link rdf:resource="#item_144"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models</dc:title>
        <dcterms:abstract>Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).</dcterms:abstract>
        <dc:date>2018-11-02</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1805.12114</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:44:22</dcterms:dateSubmitted>
        <dc:description>arXiv:1805.12114 [cs, stat]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1805.12114</dc:identifier>
        <prism:number>arXiv:1805.12114</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_142">
        <rdf:value>Comment: NIPS 2018, video and code available at https://sites.google.com/view/drl-in-a-handful-of-trials/</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_143">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/143/Chua et al. - 2018 - Deep Reinforcement Learning in a Handful of Trials.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1805.12114.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:44:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_144">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/144/1805.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1805.12114</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:44:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_145">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deisenroth</foaf:surname>
                        <foaf:givenName>Marc</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rasmussen</foaf:surname>
                        <foaf:givenName>Carl</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_146"/>
        <link:link rdf:resource="#item_147"/>
        <dc:title>PILCO: A Model-Based and Data-Efficient Approach to Policy Search.</dc:title>
        <dcterms:abstract>In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.</dcterms:abstract>
        <dc:date>2011-01-01</dc:date>
        <z:shortTitle>PILCO</z:shortTitle>
        <z:libraryCatalog>ResearchGate</z:libraryCatalog>
        <bib:pages>465-472</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_146">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/146/Deisenroth and Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach t.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/profile/Marc-Deisenroth/publication/221345233_PILCO_A_Model-Based_and_Data-Efficient_Approach_to_Policy_Search/links/0c96053be54e81bc8b000000/PILCO-A-Model-Based-and-Data-Efficient-Approach-to-Policy-Search.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:48:43</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_147">
        <z:itemType>attachment</z:itemType>
        <dc:title>ResearchGate Link</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/publication/221345233_PILCO_A_Model-Based_and_Data-Efficient_Approach_to_Policy_Search</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 11:48:43</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1907.03613">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Yuxiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Caluwaerts</foaf:surname>
                        <foaf:givenName>Ken</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Iscen</foaf:surname>
                        <foaf:givenName>Atil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Tingnan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>Jie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sindhwani</foaf:surname>
                        <foaf:givenName>Vikas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_149"/>
        <link:link rdf:resource="#item_150"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Data Efficient Reinforcement Learning for Legged Robots</dc:title>
        <dcterms:abstract>We present a model-based framework for robot locomotion that achieves walking based on only 4.5 minutes (45,000 control steps) of data collected on a quadruped robot. To accurately model the robot's dynamics over a long horizon, we introduce a loss function that tracks the model's prediction over multiple timesteps. We adapt model predictive control to account for planning latency, which allows the learned model to be used for real time control. Additionally, to ensure safe exploration during model learning, we embed prior knowledge of leg trajectories into the action space. The resulting system achieves fast and robust locomotion. Unlike model-free methods, which optimize for a particular task, our planner can use the same learned dynamics for various tasks, simply by changing the reward function. To the best of our knowledge, our approach is more than an order of magnitude more sample efficient than current model-free methods.</dcterms:abstract>
        <dc:date>2019-10-06</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1907.03613</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 14:43:58</dcterms:dateSubmitted>
        <dc:description>arXiv:1907.03613 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1907.03613</dc:identifier>
        <prism:number>arXiv:1907.03613</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_149">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/149/Yang et al. - 2019 - Data Efficient Reinforcement Learning for Legged R.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1907.03613.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 14:44:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_150">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/150/1907.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1907.03613</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 14:44:05</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1812.11103">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Haarnoja</foaf:surname>
                        <foaf:givenName>Tuomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ha</foaf:surname>
                        <foaf:givenName>Sehoon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Aurick</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>Jie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tucker</foaf:surname>
                        <foaf:givenName>George</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levine</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_152"/>
        <link:link rdf:resource="#item_153"/>
        <link:link rdf:resource="#item_154"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Learning to Walk via Deep Reinforcement Learning</dc:title>
        <dcterms:abstract>Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.</dcterms:abstract>
        <dc:date>2019-06-19</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1812.11103</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 14:44:59</dcterms:dateSubmitted>
        <dc:description>arXiv:1812.11103 [cs, stat]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1812.11103</dc:identifier>
        <prism:number>arXiv:1812.11103</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_152">
        <rdf:value>Comment: RSS 2019, https://sites.google.com/view/minitaur-locomotion/</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_153">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/153/Haarnoja et al. - 2019 - Learning to Walk via Deep Reinforcement Learning.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1812.11103.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 14:45:01</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_154">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/154/1812.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1812.11103</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-01 14:45:07</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
</rdf:RDF>
